import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train.head()

train.isnull().sum()

print(train.columns)
print(test.columns)

train.drop(labels = ["Cabin", "Ticket"], axis=1, inplace = True)
test.drop(labels = ["Cabin", "Ticket"], axis=1, inplace = True)
print(train.isnull().sum())
print(test.isnull().sum())

train.head()

copy = train.copy()
copy.dropna(inplace = True)
sns.distplot(copy["Age"]);

train["Age"].fillna(train["Age"].median(), inplace = True)
test["Age"].fillna(test["Age"].median(), inplace = True) 
train["Embarked"].fillna("S", inplace = True)
test["Fare"].fillna(test["Fare"].median(), inplace = True)
train.isnull().sum()

sns.barplot(x="Pclass", y="Survived", data=train)
plt.ylabel("Survival Rate")
plt.title("Distribution of Survival Based on Class")
plt.show()

sns.barplot(x="Pclass", y="Survived", hue="Sex", data=train)
plt.ylabel("Survival Rate")
plt.title("Survival Rates Based on Gender and Class");

sns.barplot(x="Sex", y="Survived", hue="Pclass", data=train)
plt.ylabel("Survival Rate")
plt.title("Survival Rates Based on Gender and Class");

sns.pairplot(train);

train.loc[train["Sex"] == "male", "Sex"] = 0
train.loc[train["Sex"] == "female", "Sex"] = 1


train.loc[train["Embarked"] == "S", "Embarked"] = 0
train.loc[train["Embarked"] == "C", "Embarked"] = 1
train.loc[train["Embarked"] == "Q", "Embarked"] = 2

test.loc[test["Sex"] == "male", "Sex"] = 0
test.loc[test["Sex"] == "female", "Sex"] = 1


test.loc[test["Embarked"] == "S", "Embarked"] = 0
test.loc[test["Embarked"] == "C", "Embarked"] = 1
test.loc[test["Embarked"] == "Q", "Embarked"] = 2

train["FamSize"] = train["SibSp"] + train["Parch"] + 1
test["FamSize"] = test["SibSp"] + test["Parch"] + 1

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import GridSearchCV

features = ["Pclass", "Sex", "Age", "Embarked", "Fare", "FamSize"]
X_train = train[features]
y_train = train["Survived"]
X_test = test[features]

from sklearn.model_selection import train_test_split


X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) 

rf_clf = RandomForestClassifier()
rf_clf.fit(X_training, y_training)
pred_rf = rf_clf.predict(X_valid)
acc_rf = accuracy_score(y_valid, pred_rf)

print(acc_rf)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_valid, rf_clf.predict(X_valid))
cm

sns.set()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["0", "1"], yticklabels=["0", "1"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

model_performance = pd.DataFrame({
    "Model": ["Random Forest"],#, "XGBClassifier"],
    "Accuracy": [ acc_rf]#, acc_xg]
})

model_performance.sort_values(by="Accuracy", ascending=False)

rf_clf = RandomForestClassifier()

parameters = {"n_estimators": [4, 5, 6, 7, 8, 9, 10, 15], 
              "criterion": ["gini", "entropy"], 
              "max_depth": [2, 3, 5, 10], 
              "min_samples_split": [2, 3, 5, 10],
              "min_samples_leaf": [1, 5, 8, 10]
             }
             
grid_cv = GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))
grid_cv = grid_cv.fit(X_train, y_train)

print("Our optimized Random Forest model is:")
grid_cv.best_estimator_

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn import linear_model, decomposition, datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets

logistic_Reg = LogisticRegression()
n_components = list(range(1,X_training.shape[1]+1,1))
C = np.logspace(-4, 4, 50)
penalty = ['l1', 'l2']
parameters = dict(pca__n_components=n_components,
                      logistic_Reg__C=C,
                      logistic_Reg__penalty=penalty)
grid_cv2 = GridSearchCV(logistic_Reg, parameters)
grid_cv2 = grid_cv.fit(X_train, y_train)
grid_cv.best_estimator_

optimal_model = grid_cv.best_estimator_
optimal_model.fit(X_train, y_train)
predict_optimal = optimal_model.predict(X_valid)
final_accuracy = accuracy_score(y_valid, predict_optimal)
print(final_accuracy)

from sklearn.model_selection import learning_curve
N, train_score, val_score = learning_curve(rf_clf, X_train, y_train, train_sizes= np.linspace(0.1,1,10), cv=5)

N, train_score, val_score = learning_curve(rf_clf, X_train, y_train, train_sizes= np.linspace(0.1,1,10), cv=5)
print(N)
plt.plot(N, train_score.mean(axis=1), label='train')
plt.plot(N, val_score.mean(axis=1), label='validation')
plt.xlabel('train_sizes')
plt.legend();
